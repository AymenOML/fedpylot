{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrmnNRO1CZE2"
   },
   "source": [
    "## License Information\n",
    "\n",
    "Copyright (C) 2023 Cyprien QuÃ©mÃ©neur\n",
    "\n",
    "For the full license, please refer to the LICENSE file in the root directory of this project.\n",
    "\n",
    "For the full copyright notices, please refer to the NOTICE file in the root directory of this project."
   ],
   "id": "hrmnNRO1CZE2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIUi5fOlYMxA"
   },
   "source": [
    "## System Information"
   ],
   "id": "EIUi5fOlYMxA"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36-qY6G5YFcJ",
    "outputId": "ecace91d-b70b-464f-9e81-b833ac8e2d6b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0) \n",
      "Pytorch version: 2.1.0+cu118 \n",
      "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-d42b298e-c248-eb89-3c6d-15d68a2461cc)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(f\"Python version: {sys.version}, {sys.version_info} \")\n",
    "print(f\"Pytorch version: {torch.__version__} \")\n",
    "!nvidia-smi -L"
   ],
   "id": "36-qY6G5YFcJ"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsbBLnP0Wa1m"
   },
   "source": [
    "## Preparation"
   ],
   "id": "bsbBLnP0Wa1m"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "99a739c9-6055-4bff-8a47-0cc5eae9649e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/MasterThesis\n"
     ]
    }
   ],
   "source": [
    "# for now the code, fully trained weights, and data are imported from a drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "!cp -r gdrive/MyDrive/fedyolo-iov ./\n",
    "drive.flush_and_unmount()\n",
    "%cd fedyolo-iov"
   ],
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7x9NRKQRC3mA"
   },
   "source": [
    "## Benchmarking"
   ],
   "id": "7x9NRKQRC3mA"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sKy8RN4aoQ6v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import output\n",
    "import pandas as pd"
   ],
   "id": "sKy8RN4aoQ6v"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rgQoj6WHoVUU"
   },
   "outputs": [],
   "source": [
    "def run(data, bsz, trials, weights, spath, img, conf, iou):\n",
    "    \"\"\"Evaluate YOLOv7(-X) several times and print the mean inference speed.\"\"\"\n",
    "    if not os.path.exists(saving_path):\n",
    "        os.makedirs(saving_path)\n",
    "    for trial in range(trials):\n",
    "        output.clear()  # only display the current trial\n",
    "        !python yolov7/test.py --data $data --batch $bsz --kround $trial --weights $weights --saving-path $spath --project $spath --img $img --conf $conf --iou $iou --device 0\n",
    "    results = pd.read_csv(os.path.join(saving_path, 'results.csv'))\n",
    "    print('\\n--------------------------------------------------------\\n')\n",
    "    if bsz == 1:\n",
    "        t = round(1000 / results['batch avg time (ms)'].mean())\n",
    "        print(f'Mean inference speed ({trials} trials, NMS excluded): {t} FPS')\n",
    "    else:\n",
    "        t = round(results['batch avg time (ms)'].mean(), 1)\n",
    "        print(f'Mean inference speed ({trials} trials, NMS excluded): {t} ms')"
   ],
   "id": "rgQoj6WHoVUU"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF06-psyWhuN"
   },
   "source": [
    "### KITTI"
   ],
   "id": "oF06-psyWhuN"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mwQMltyqTKi8"
   },
   "outputs": [],
   "source": [
    "# untar KITTI validation set\n",
    "!tar -xf datasets/kitti/server.tar -C datasets/kitti"
   ],
   "id": "mwQMltyqTKi8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uG_Km16AU0Rz"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "data = 'data/kitti.yaml'       # *.data path\n",
    "bsz = 1                        # batch size\n",
    "trials = 50                    # number of inferences to run\n",
    "img_size = 640                 # inference size (pixels)\n",
    "conf_thres = 0.001             # object confidence threshold\n",
    "iou_thres = 0.65               # IOU threshold for NMS"
   ],
   "id": "uG_Km16AU0Rz"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZs31ze3Uvx9",
    "outputId": "2a11d6bc-deac-4d5d-ceb3-60363feaa3e5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(saving_path='datasets/kitti/yolov7-bsz1', kround=49, weights=['weights/trained/kitti-yolov7-fedavgm.pt'], data='data/kitti.yaml', batch_size=1, img_size=640, conf_thres=0.001, iou_thres=0.65, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='datasets/kitti/yolov7-bsz1', name='exp', exist_ok=False, no_trace=False, v5_metric=False)\n",
      "YOLOR ðŸš€ 2023-12-10 torch 2.1.0+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16150.875MB)\n",
      "\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "Model Summary: 306 layers, 36517621 parameters, 36517621 gradients\n",
      " Convert model to Traced-model... \n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:844: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if param.grad is not None:\n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning 'datasets/kitti/server/labels.cache' images and labels... 1871 found, 0 missing, 0 empty, 0 corrupted: 100% 1871/1871 [00:00<?, ?it/s]\n",
      "                               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 1871/1871 [01:10<00:00, 26.65it/s]\n",
      "                                 all        1871       10210        0.94       0.868        0.91       0.676\n",
      "                                 Car        1871        7110        0.94        0.94       0.962       0.781\n",
      "                                 Van        1871         780        0.94       0.924       0.951       0.766\n",
      "                               Truck        1871         307       0.985       0.971       0.986       0.821\n",
      "                          Pedestrian        1871        1151       0.915       0.783       0.838       0.483\n",
      "                      Person_sitting        1871          53       0.921        0.66       0.787       0.534\n",
      "                             Cyclist        1871         428        0.94       0.848       0.899       0.623\n",
      "                                Tram        1871         148        0.95       0.919       0.925       0.694\n",
      "                                Misc        1871         233       0.931       0.901       0.935       0.704\n",
      "Speed: 8.0/1.3/9.3 ms inference/NMS/total per 640x640 image at batch-size 1\n",
      "Results saved to datasets/kitti/yolov7-bsz1/exp50\n",
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "Mean inference speed (50 trials, NMS excluded): 126 FPS\n"
     ]
    }
   ],
   "source": [
    "# YOLOv7\n",
    "weights = 'weights/trained/kitti-yolov7-fedavgm.pt'\n",
    "saving_path = 'datasets/kitti/yolov7-bsz1'\n",
    "\n",
    "run(data, bsz, trials, weights, saving_path, img_size, conf_thres, iou_thres)"
   ],
   "id": "FZs31ze3Uvx9"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v54aGiUuw6ic",
    "outputId": "0017ab7b-944c-4a30-a855-31b45f2affb3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(saving_path='datasets/kitti/yolov7x-bsz1', kround=49, weights=['weights/trained/kitti-yolov7x-fedavgm.pt'], data='data/kitti.yaml', batch_size=1, img_size=640, conf_thres=0.001, iou_thres=0.65, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='datasets/kitti/yolov7x-bsz1', name='exp', exist_ok=False, no_trace=False, v5_metric=False)\n",
      "YOLOR ðŸš€ 2023-12-10 torch 2.1.0+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16150.875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 354 layers, 70827253 parameters, 70827253 gradients\n",
      " Convert model to Traced-model... \n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:844: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if param.grad is not None:\n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning 'datasets/kitti/server/labels.cache' images and labels... 1871 found, 0 missing, 0 empty, 0 corrupted: 100% 1871/1871 [00:00<?, ?it/s]\n",
      "                               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 1871/1871 [01:13<00:00, 25.58it/s]\n",
      "                                 all        1871       10210       0.931       0.884       0.916       0.686\n",
      "                                 Car        1871        7110       0.936       0.953       0.964       0.792\n",
      "                                 Van        1871         780       0.945        0.94       0.962       0.784\n",
      "                               Truck        1871         307       0.969        0.98       0.989       0.826\n",
      "                          Pedestrian        1871        1151       0.922       0.815       0.863       0.508\n",
      "                      Person_sitting        1871          53        0.88        0.69       0.794       0.514\n",
      "                             Cyclist        1871         428       0.946       0.846        0.89       0.626\n",
      "                                Tram        1871         148       0.908       0.939       0.915       0.725\n",
      "                                Misc        1871         233       0.947        0.91       0.946       0.716\n",
      "Speed: 9.2/1.3/10.5 ms inference/NMS/total per 640x640 image at batch-size 1\n",
      "Results saved to datasets/kitti/yolov7x-bsz1/exp50\n",
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "Mean inference speed (50 trials, NMS excluded): 110 FPS\n"
     ]
    }
   ],
   "source": [
    "# YOLOv7-X\n",
    "weights = 'weights/trained/kitti-yolov7x-fedavgm.pt'\n",
    "saving_path = 'datasets/kitti/yolov7x-bsz1'\n",
    "\n",
    "run(data, bsz, trials, weights, saving_path, img_size, conf_thres, iou_thres)"
   ],
   "id": "v54aGiUuw6ic"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE2UdEUAWkKt"
   },
   "source": [
    "### nuImages-10"
   ],
   "id": "yE2UdEUAWkKt"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mhjW935XWl_M"
   },
   "outputs": [],
   "source": [
    "# untar nuImages-10 validation set\n",
    "!tar -xf datasets/nuimages10/server.tar -C datasets/nuimages10"
   ],
   "id": "mhjW935XWl_M"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Qya7R8JHav6G"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "data = 'data/nuimages10.yaml'  # *.data path\n",
    "bsz = 1                        # batch size\n",
    "trials = 10                    # number of inferences to run\n",
    "img_size = 640                 # inference size (pixels)\n",
    "conf_thres = 0.001             # object confidence threshold\n",
    "iou_thres = 0.65               # IOU threshold for NMS"
   ],
   "id": "Qya7R8JHav6G"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tJilPLpwRM2G",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fc8cbdac-5b83-4c89-903f-fa0431101437"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(saving_path='datasets/nuimages10/yolov7-bsz1', kround=9, weights=['weights/trained/nuimages10-yolov7-fedavgm.pt'], data='data/nuimages10.yaml', batch_size=1, img_size=640, conf_thres=0.001, iou_thres=0.65, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='datasets/nuimages10/yolov7-bsz1', name='exp', exist_ok=False, no_trace=False, v5_metric=False)\n",
      "YOLOR ðŸš€ 2023-12-10 torch 2.1.0+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16150.875MB)\n",
      "\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "Model Summary: 306 layers, 36528391 parameters, 36528391 gradients\n",
      " Convert model to Traced-model... \n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:844: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if param.grad is not None:\n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning 'datasets/nuimages10/server/labels.cache' images and labels... 16445 found, 0 missing, 1661 empty, 0 corrupted: 100% 16445/16445 [00:00<?, ?it/s]\n",
      "                               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 16445/16445 [14:02<00:00, 19.52it/s]\n",
      "                                 all       16445      133465       0.817       0.652       0.725       0.476\n",
      "                          pedestrian       16445       32185       0.868       0.677       0.771       0.446\n",
      "                             barrier       16445       18433       0.836       0.678       0.764       0.488\n",
      "                        traffic_cone       16445       18587       0.875       0.761       0.835       0.478\n",
      "                             bicycle       16445        3352       0.788       0.713       0.763        0.53\n",
      "                                 bus       16445        1885       0.835       0.616       0.715       0.544\n",
      "                                 car       16445       47279       0.867       0.815       0.881       0.637\n",
      "                construction_vehicle       16445        1303       0.777       0.599       0.647       0.344\n",
      "                          motorcycle       16445        3097       0.828       0.815       0.859       0.586\n",
      "                             trailer       16445         486       0.712       0.212       0.309       0.198\n",
      "                               truck       16445        6858       0.782        0.63       0.705       0.513\n",
      "Speed: 8.1/1.4/9.5 ms inference/NMS/total per 640x640 image at batch-size 1\n",
      "Results saved to datasets/nuimages10/yolov7-bsz1/exp10\n",
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "Mean inference speed (10 trials, NMS excluded): 122 FPS\n"
     ]
    }
   ],
   "source": [
    "# YOLOv7\n",
    "weights = 'weights/trained/nuimages10-yolov7-fedavgm.pt'\n",
    "saving_path = 'datasets/nuimages10/yolov7-bsz1'\n",
    "\n",
    "run(data, bsz, trials, weights, saving_path, img_size, conf_thres, iou_thres)"
   ],
   "id": "tJilPLpwRM2G"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MyKQJKwrRNyE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d9343975-59c8-420e-d55e-078f972bae2f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(saving_path='datasets/nuimages10/yolov7x-bsz1', kround=9, weights=['weights/trained/nuimages10-yolov7x-fedavgm.pt'], data='data/nuimages10.yaml', batch_size=1, img_size=640, conf_thres=0.001, iou_thres=0.65, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='datasets/nuimages10/yolov7x-bsz1', name='exp', exist_ok=False, no_trace=False, v5_metric=False)\n",
      "YOLOR ðŸš€ 2023-12-10 torch 2.1.0+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16150.875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 354 layers, 70840711 parameters, 70840711 gradients\n",
      " Convert model to Traced-model... \n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:844: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if param.grad is not None:\n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning 'datasets/nuimages10/server/labels.cache' images and labels... 16445 found, 0 missing, 1661 empty, 0 corrupted: 100% 16445/16445 [00:00<?, ?it/s]\n",
      "                               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 16445/16445 [14:37<00:00, 18.74it/s]\n",
      "                                 all       16445      133465       0.817       0.673       0.743       0.494\n",
      "                          pedestrian       16445       32185       0.874       0.693       0.783       0.461\n",
      "                             barrier       16445       18433       0.841       0.704       0.783       0.504\n",
      "                        traffic_cone       16445       18587        0.88       0.773       0.844       0.489\n",
      "                             bicycle       16445        3352       0.778       0.734       0.774        0.54\n",
      "                                 bus       16445        1885        0.82        0.64       0.727       0.558\n",
      "                                 car       16445       47279       0.874       0.824       0.892        0.65\n",
      "                construction_vehicle       16445        1303       0.771       0.635       0.667       0.367\n",
      "                          motorcycle       16445        3097       0.839       0.826        0.87       0.603\n",
      "                             trailer       16445         486       0.688       0.263       0.367       0.238\n",
      "                               truck       16445        6858       0.801       0.638       0.726        0.53\n",
      "Speed: 9.6/1.4/11.1 ms inference/NMS/total per 640x640 image at batch-size 1\n",
      "Results saved to datasets/nuimages10/yolov7x-bsz1/exp10\n",
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "Mean inference speed (10 trials, NMS excluded): 103 FPS\n"
     ]
    }
   ],
   "source": [
    "# YOLOv7-X\n",
    "weights = 'weights/trained/nuimages10-yolov7x-fedavgm.pt'\n",
    "saving_path = 'datasets/nuimages10/yolov7x-bsz1'\n",
    "\n",
    "run(data, bsz, trials, weights, saving_path, img_size, conf_thres, iou_thres)"
   ],
   "id": "MyKQJKwrRNyE"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwUTTVCXbVz9"
   },
   "source": [
    "### nuImages-23"
   ],
   "id": "RwUTTVCXbVz9"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QfFUI506Wneg"
   },
   "outputs": [],
   "source": [
    "# untar nuImages-23 validation set\n",
    "!tar -xf datasets/nuimages23/server.tar -C datasets/nuimages23"
   ],
   "id": "QfFUI506Wneg"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BNDqU9gta8Mx"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "data = 'data/nuimages23.yaml'  # *.data path\n",
    "bsz = 1                        # batch size\n",
    "trials = 10                    # number of inferences to run\n",
    "img_size = 640                 # inference size (pixels)\n",
    "conf_thres = 0.001             # object confidence threshold\n",
    "iou_thres = 0.65               # IOU threshold for NMS"
   ],
   "id": "BNDqU9gta8Mx"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "G26b_zDlXX4r",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "44a60f52-7154-48ff-85f3-795f7b3f49ab"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(saving_path='datasets/nuimages23/yolov7-bsz1', kround=9, weights=['weights/trained/nuimages23-yolov7-fedavgm.pt'], data='data/nuimages23.yaml', batch_size=1, img_size=640, conf_thres=0.001, iou_thres=0.65, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='datasets/nuimages23/yolov7-bsz1', name='exp', exist_ok=False, no_trace=False, v5_metric=False)\n",
      "YOLOR ðŸš€ 2023-12-10 torch 2.1.0+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16150.875MB)\n",
      "\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "Model Summary: 306 layers, 36598396 parameters, 36598396 gradients\n",
      " Convert model to Traced-model... \n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:844: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if param.grad is not None:\n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning 'datasets/nuimages23/server/labels.cache' images and labels... 16445 found, 0 missing, 1561 empty, 0 corrupted: 100% 16445/16445 [00:00<?, ?it/s]\n",
      "                               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 16445/16445 [14:39<00:00, 18.69it/s]\n",
      "                                 all       16445      136073       0.694       0.445       0.454       0.285\n",
      "                              animal       16445          82       0.498       0.183       0.152      0.0659\n",
      "              human.pedestrian.adult       16445       28721        0.77       0.719       0.761        0.44\n",
      "              human.pedestrian.child       16445         251       0.487       0.104       0.105      0.0546\n",
      "human.pedestrian.construction_worker       16445        3117       0.742       0.568        0.61       0.321\n",
      "  human.pedestrian.personal_mobility       16445         453       0.477       0.629       0.502       0.225\n",
      "     human.pedestrian.police_officer       16445          96       0.509       0.281       0.247       0.167\n",
      "           human.pedestrian.stroller       16445          70       0.728       0.268       0.358       0.243\n",
      "         human.pedestrian.wheelchair       16445           2           1           0           0           0\n",
      "              movable_object.barrier       16445       18433       0.765       0.726       0.764       0.488\n",
      "               movable_object.debris       16445         710       0.515       0.141       0.171      0.0969\n",
      "    movable_object.pushable_pullable       16445         645       0.557       0.482       0.428       0.266\n",
      "          movable_object.trafficcone       16445       18587       0.807       0.804       0.832       0.474\n",
      "          static_object.bicycle_rack       16445         603       0.572       0.657       0.574       0.338\n",
      "                     vehicle.bicycle       16445        3352       0.705       0.748        0.76       0.522\n",
      "                   vehicle.bus.bendy       16445          62       0.392      0.0484      0.0618      0.0419\n",
      "                   vehicle.bus.rigid       16445        1823       0.738       0.659       0.708       0.538\n",
      "                         vehicle.car       16445       47279        0.81       0.846       0.881       0.635\n",
      "                vehicle.construction       16445        1303       0.729       0.613       0.635       0.341\n",
      "         vehicle.emergency.ambulance       16445           8           1           0           0           0\n",
      "            vehicle.emergency.police       16445          35           1           0    0.000764    0.000707\n",
      "                  vehicle.motorcycle       16445        3097       0.766       0.837       0.857       0.584\n",
      "                     vehicle.trailer       16445         486        0.66       0.257       0.332       0.196\n",
      "                       vehicle.truck       16445        6858       0.724       0.665       0.707       0.512\n",
      "Speed: 8.7/1.5/10.2 ms inference/NMS/total per 640x640 image at batch-size 1\n",
      "Results saved to datasets/nuimages23/yolov7-bsz1/exp10\n",
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "Mean inference speed (10 trials, NMS excluded): 118 FPS\n"
     ]
    }
   ],
   "source": [
    "# YOLOv7\n",
    "weights = 'weights/trained/nuimages23-yolov7-fedavgm.pt'\n",
    "saving_path = 'datasets/nuimages23/yolov7-bsz1'\n",
    "\n",
    "run(data, bsz, trials, weights, saving_path, img_size, conf_thres, iou_thres)"
   ],
   "id": "G26b_zDlXX4r"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iiQ48vXGXcme",
    "outputId": "ff765b9e-0c41-4972-8920-3a4b8de1e1a5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(saving_path='datasets/nuimages23/yolov7x-bsz1', kround=9, weights=['weights/trained/nuimages23-yolov7x-fedavgm.pt'], data='data/nuimages23.yaml', batch_size=1, img_size=640, conf_thres=0.001, iou_thres=0.65, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='datasets/nuimages23/yolov7x-bsz1', name='exp', exist_ok=False, no_trace=False, v5_metric=False)\n",
      "YOLOR ðŸš€ 2023-12-10 torch 2.1.0+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16150.875MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 354 layers, 70928188 parameters, 70928188 gradients\n",
      " Convert model to Traced-model... \n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:844: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if param.grad is not None:\n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning 'datasets/nuimages23/server/labels.cache' images and labels... 16445 found, 0 missing, 1561 empty, 0 corrupted: 100% 16445/16445 [00:00<?, ?it/s]\n",
      "                               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 16445/16445 [14:50<00:00, 18.47it/s]\n",
      "                                 all       16445      136073       0.705       0.448       0.465       0.296\n",
      "                              animal       16445          82       0.523      0.0854       0.137      0.0704\n",
      "              human.pedestrian.adult       16445       28721       0.798       0.731       0.776       0.461\n",
      "              human.pedestrian.child       16445         251       0.462      0.0876       0.114      0.0621\n",
      "human.pedestrian.construction_worker       16445        3117       0.755       0.593       0.636       0.345\n",
      "  human.pedestrian.personal_mobility       16445         453       0.521       0.605       0.506       0.236\n",
      "     human.pedestrian.police_officer       16445          96       0.444       0.229       0.254       0.162\n",
      "           human.pedestrian.stroller       16445          70       0.689       0.386        0.39       0.266\n",
      "         human.pedestrian.wheelchair       16445           2           1           0     0.00133    0.000133\n",
      "              movable_object.barrier       16445       18433       0.785       0.734       0.776       0.499\n",
      "               movable_object.debris       16445         710       0.472       0.162       0.166      0.0915\n",
      "    movable_object.pushable_pullable       16445         645       0.554       0.489       0.439       0.279\n",
      "          movable_object.trafficcone       16445       18587       0.838       0.801       0.845       0.487\n",
      "          static_object.bicycle_rack       16445         603       0.568       0.655       0.576       0.344\n",
      "                     vehicle.bicycle       16445        3352       0.733       0.757       0.771       0.537\n",
      "                   vehicle.bus.bendy       16445          62       0.573      0.0484      0.0871      0.0583\n",
      "                   vehicle.bus.rigid       16445        1823       0.769       0.669       0.728       0.559\n",
      "                         vehicle.car       16445       47279        0.83       0.848       0.886       0.645\n",
      "                vehicle.construction       16445        1303       0.712       0.652       0.656       0.358\n",
      "         vehicle.emergency.ambulance       16445           8           1           0           0           0\n",
      "            vehicle.emergency.police       16445          35           1           0    0.000373    0.000296\n",
      "                  vehicle.motorcycle       16445        3097       0.786       0.843       0.867       0.599\n",
      "                     vehicle.trailer       16445         486       0.646       0.272        0.36       0.217\n",
      "                       vehicle.truck       16445        6858        0.75       0.663       0.719       0.522\n",
      "Speed: 10.0/1.5/11.4 ms inference/NMS/total per 640x640 image at batch-size 1\n",
      "Results saved to datasets/nuimages23/yolov7x-bsz1/exp10\n",
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "Mean inference speed (10 trials, NMS excluded): 101 FPS\n"
     ]
    }
   ],
   "source": [
    "# YOLOv7-X\n",
    "weights = 'weights/trained/nuimages23-yolov7x-fedavgm.pt'\n",
    "saving_path = 'datasets/nuimages23/yolov7x-bsz1'\n",
    "\n",
    "run(data, bsz, trials, weights, saving_path, img_size, conf_thres, iou_thres)"
   ],
   "id": "iiQ48vXGXcme"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3-lE-U56-R6"
   },
   "outputs": [],
   "source": [],
   "id": "o3-lE-U56-R6"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "V100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
